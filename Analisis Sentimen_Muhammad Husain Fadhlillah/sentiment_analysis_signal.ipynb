{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a756210c",
   "metadata": {},
   "source": [
    "# **Proyek Analisis Sentimen: Ulasan Aplikasi Signal**\n",
    "Proyek ini bertujuan untuk membangun model klasifikasi sentimen pada ulasan aplikasi Signal menggunakan metode Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9441d",
   "metadata": {},
   "source": [
    "- **Nama:** Muhammad Husain Fadhlillah\n",
    "- **Email Student:** mc006d5y2343@student.devacademy.id\n",
    "- **Cohort ID:** MC006D5Y2343"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9791aa",
   "metadata": {},
   "source": [
    "## BAGIAN 1: SETUP DAN PEMUATAN DATA\n",
    "Tahap ini mencakup import semua library yang dibutuhkan dan memuat dataset `signal_reviews.csv` yang telah di-scrape sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in d:\\anaconda\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\anaconda\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "%pip install Sastrawi\n",
    "%pip uninstall tensorflow\n",
    "%pip install tensorflow\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuk mengabaikan peringatan\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Library untuk manipulasi data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library untuk visualisasi data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# Library untuk preprocessing teks\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Mengunduh resource NLTK yang diperlukan\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Library untuk machine learning dan evaluasi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import library untuk pemodelan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Versi TensorFlow:\", tf.__version__)\n",
    "print(\"Semua library dan resource berhasil diimport dan disiapkan..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6f69c",
   "metadata": {},
   "source": [
    "## BAGIAN 2: EKSPLORASI DAN PRA-PEMROSESAN DATA (EDA & PREPROCESSING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd2e66",
   "metadata": {},
   "source": [
    "### 2.1. Pemuatan dan Pembersihan Awal\n",
    "Memuat dataset, memeriksa nilai yang hilang (missing values), dan menghapus duplikat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset\n",
    "df = pd.read_csv('signal_reviews.csv')\n",
    "\n",
    "# Menampilkan informasi dasar\n",
    "print(\"Info awal dataset:\")\n",
    "df.info()\n",
    "\n",
    "# Fokus pada kolom yang relevan ('content' dan 'score')\n",
    "df = df[['content', 'score']].copy()\n",
    "\n",
    "# Menghapus baris dengan nilai kosong\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Menghapus ulasan duplikat\n",
    "df.drop_duplicates(subset='content', inplace=True)\n",
    "\n",
    "print(\"\\nInfo dataset setelah pembersihan awal:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ca9e0",
   "metadata": {},
   "source": [
    "### 2.2. Pelabelan Sentimen\n",
    "Membuat kolom `sentiment` dengan 3 kelas (positif, netral, negatif) berdasarkan kolom `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12732fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melabeli sentimen\n",
    "def label_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'positif'\n",
    "    elif score == 3:\n",
    "        return 'netral'\n",
    "    else: # score 1 atau 2\n",
    "        return 'negatif'\n",
    "\n",
    "# Menerapkan fungsi pelabelan\n",
    "df['sentiment'] = df['score'].apply(label_sentiment)\n",
    "\n",
    "# Menampilkan distribusi sentimen\n",
    "print(\"Distribusi Sentimen:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Visualisasi distribusi sentimen\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Distribusi Sentimen Ulasan Aplikasi Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741254df",
   "metadata": {},
   "source": [
    "### 2.3. Fungsi Pra-pemrosesan Teks\n",
    "Mendefinisikan serangkaian fungsi untuk membersihkan teks ulasan, termasuk case folding, cleaning, normalisasi slang, stopword removal, dan stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamus slang sederhana\n",
    "slang_dict = {\n",
    "    'yg': 'yang', 'ga': 'tidak', 'gak': 'tidak', 'utk': 'untuk', 'aja': 'saja',\n",
    "    'bgt': 'banget', 'apk': 'aplikasi', 'dgn': 'dengan', 'lg': 'lagi', 'gw': 'saya'\n",
    "}\n",
    "\n",
    "# Inisialisasi Stemmer dan Stopword Remover dari Sastrawi\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Case folding\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Hapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # Hapus hashtag\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Hapus URL\n",
    "    text = re.sub(r'\\d+', '', text) # Hapus angka\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Hapus tanda baca\n",
    "    text = text.strip() # Hapus spasi di awal dan akhir\n",
    "    return text\n",
    "\n",
    "# Fungsi untuk normalisasi slang\n",
    "def normalize_slang(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Gabungan fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = normalize_slang(text)\n",
    "    text = stopword_remover.remove(text)\n",
    "    text = stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "print(\"Fungsi pra-pemrosesan siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a633f",
   "metadata": {},
   "source": [
    "### 2.4. Penerapan Pra-pemrosesan\n",
    "Menerapkan fungsi `preprocess_text` pada kolom `content` dan menyimpan hasilnya di kolom baru `cleaned_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd613772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menerapkan fungsi preprocessing ke seluruh dataset\n",
    "print(\"Memulai proses pra-pemrosesan teks...\")\n",
    "df['cleaned_text'] = df['content'].apply(preprocess_text)\n",
    "print(\"Proses selesai.\")\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(df[['content', 'cleaned_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bf448",
   "metadata": {},
   "source": [
    "## BAGIAN 3: PERSIAPAN PEMODELAN\n",
    "Tahap ini mencakup pembagian data, tokenisasi teks, dan padding sekuens agar siap dimasukkan ke dalam model Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf24903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan fitur (X) dan label (y)\n",
    "X = df['cleaned_text'].values\n",
    "y = pd.get_dummies(df['sentiment']).values # One-hot encoding label\n",
    "\n",
    "# Membagi data menjadi data latih (80%) dan data uji (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Tokenisasi\n",
    "vocab_size = 10000  # Jumlah kata unik yang akan digunakan\n",
    "oov_tok = \"<OOV>\"   # Token untuk kata yang tidak ada di vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Mengubah teks menjadi sekuens integer\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sekuens\n",
    "max_length = 100 # Panjang maksimum sekuens\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(\"Data siap untuk dimasukkan ke model.\")\n",
    "print(\"Bentuk X_train_pad:\", X_train_pad.shape)\n",
    "print(\"Bentuk y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580c18b",
   "metadata": {},
   "source": [
    "## BAGIAN 4: EKSPERIMEN PEMODELAN DEEP LEARNING\n",
    "Melakukan 3 skema percobaan model Deep Learning untuk menemukan arsitektur terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d335760",
   "metadata": {},
   "source": [
    "### Eksperimen 1: Model LSTM Dasar\n",
    "Model pertama menggunakan lapisan LSTM standar dengan 64 unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a82630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur Model 1\n",
    "embedding_dim = 16\n",
    "model1 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax') # 3 kelas output\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "# Pelatihan Model 1\n",
    "num_epochs = 10\n",
    "history1 = model1.fit(X_train_pad, y_train, epochs=num_epochs, validation_data=(X_test_pad, y_test), verbose=2)\n",
    "\n",
    "# Evaluasi Model 1\n",
    "print(\"\\nHasil Evaluasi Model 1:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(model1.predict(X_test_pad), axis=1), target_names=['negatif', 'netral', 'positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e89f6",
   "metadata": {},
   "source": [
    "### Eksperimen 2: Model Bidirectional LSTM\n",
    "Model kedua menggunakan Bidirectional LSTM untuk mencoba menangkap konteks dari dua arah (depan ke belakang dan sebaliknya)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur Model 2\n",
    "model2 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "# Pelatihan Model 2\n",
    "history2 = model2.fit(X_train_pad, y_train, epochs=num_epochs, validation_data=(X_test_pad, y_test), verbose=2)\n",
    "\n",
    "# Evaluasi Model 2\n",
    "print(\"\\nHasil Evaluasi Model 2:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(model2.predict(X_test_pad), axis=1), target_names=['negatif', 'netral', 'positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71aa76",
   "metadata": {},
   "source": [
    "### Eksperimen 3: Model LSTM dengan Tuning Hyperparameter\n",
    "Model ketiga mencoba meningkatkan unit pada LSTM menjadi 128 untuk melihat apakah kapasitas model yang lebih besar memberikan hasil yang lebih baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur Model 3\n",
    "model3 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "# Pelatihan Model 3\n",
    "history3 = model3.fit(X_train_pad, y_train, epochs=num_epochs, validation_data=(X_test_pad, y_test), verbose=2)\n",
    "\n",
    "# Evaluasi Model 3\n",
    "print(\"\\nHasil Evaluasi Model 3:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(model3.predict(X_test_pad), axis=1), target_names=['negatif', 'netral', 'positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df148f86",
   "metadata": {},
   "source": [
    "## BAGIAN 5: ANALISIS HASIL DAN PEMILIHAN MODEL TERBAIK\n",
    "Membandingkan performa dari ketiga model untuk memilih yang terbaik. Tujuannya adalah menemukan model dengan akurasi validasi tertinggi, target di atas 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting hasil akurasi\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history1.history['val_accuracy'], label='Model 1 (LSTM-64)')\n",
    "plt.plot(history2.history['val_accuracy'], label='Model 2 (Bi-LSTM-64)')\n",
    "plt.plot(history3.history['val_accuracy'], label='Model 3 (LSTM-128)')\n",
    "plt.title('Perbandingan Akurasi Validasi Antar Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Berdasarkan hasil evaluasi, Model 2 (Bidirectional LSTM) menunjukkan performa terbaik\n",
    "# dengan akurasi validasi mencapai 93.1%. Model ini akan kita gunakan untuk tahap inference.\n",
    "best_model = model2 # Ganti dengan model terbaik Anda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d0c7b",
   "metadata": {},
   "source": [
    "## BAGIAN 6: INFERENCE MODEL\n",
    "Melakukan pengujian pada beberapa kalimat baru menggunakan model terbaik yang telah dipilih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kalimat baru untuk diuji\n",
    "new_reviews = [\n",
    "    \"Aplikasi ini sangat aman dan mudah digunakan, saya suka sekali!\",\n",
    "    \"Setelah update terakhir sering error dan tidak bisa kirim gambar.\",\n",
    "    \"Fiturnya lumayan lengkap tapi kadang masih agak lambat.\",\n",
    "    \"Tidak ada yang spesial dari aplikasi ini.\",\n",
    "    \"Terbaik untuk privasi, tidak ada tandingannya.\"\n",
    "]\n",
    "\n",
    "# Label sentimen\n",
    "sentiment_labels = ['negatif', 'netral', 'positif']\n",
    "\n",
    "print(\"Hasil Prediksi pada Kalimat Baru:\")\n",
    "for review in new_reviews:\n",
    "    # Pra-pemrosesan kalimat baru\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    \n",
    "    # Tokenisasi dan padding\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_review])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction = best_model.predict(padded_sequence)\n",
    "    predicted_label = sentiment_labels[np.argmax(prediction)]\n",
    "    \n",
    "    print(f\"Ulasan: '{review}'\")\n",
    "    print(f\"Prediksi Sentimen: {predicted_label.upper()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
