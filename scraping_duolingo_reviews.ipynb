{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyek Analisis Sentimen: Scraping Ulasan Aplikasi Duolingo**"
      ],
      "metadata": {
        "id": "-0Kbft9S4j5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nama:** Muhammad Husain Fadhlillah\n",
        "- **Email Student:** mc006d5y2343@student.devacademy.id\n",
        "- **Cohort ID:** MC006D5Y2343\n",
        "\n",
        "Notebook ini bertujuan untuk melakukan scraping data ulasan (reviews) dari aplikasi Duolingo di Google Play Store."
      ],
      "metadata": {
        "id": "93Ykj1CU4mZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybjLFEhBSGA5",
        "outputId": "39127821-cdb8-438a-b348-4e3f02a57b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-play-scraper\n",
            "  Downloading google_play_scraper-1.2.7-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_play_scraper-1.2.7-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.2.7\n"
          ]
        }
      ],
      "source": [
        "# Instalasi library yang dibutuhkan\n",
        "!pip install google-play-scraper\n",
        "\n",
        "# Mengimpor library yang diperlukan\n",
        "from google_play_scraper import reviews_all, Sort\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Untuk mengabaikan peringatan\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "`Successfully installed google-play-scraper-1.2.7`\n",
        "\n",
        "- **Metode yang digunakan:**\n",
        "  - **Manajemen Dependensi:** Menggunakan `pip install` untuk memasang library pihak ketiga.\n",
        "  - **Impor Library:** Menggunakan `import` untuk memuat modul-modul yang diperlukan ke dalam lingkungan kerja.\n",
        "\n",
        "- **Alasan penggunaan:**\n",
        "  - `google-play-scraper`: Ini adalah *tools* utama yang dipilih untuk berinteraksi dengan Google Play Store. Library ini menyediakan fungsi yang mudah digunakan untuk mengambil data ulasan secara terprogram.\n",
        "  - `pandas`: Library standar emas untuk manipulasi dan analisis data di Python. Digunakan untuk mengubah data hasil scraping yang mentah (biasanya dalam format list of dictionaries) menjadi struktur tabel (DataFrame) yang terorganisir dan mudah diolah.\n",
        "\n",
        "- **Insight dan Hasil yang didapat:**\n",
        "  - **Output `Successfully installed`** mengonfirmasi bahwa lingkungan kerja telah siap dan semua dependensi yang diperlukan untuk scraping telah terpenuhi. Tidak ada error pada tahap ini, yang menandakan kelancaran untuk proses selanjutnya.\n",
        "  - Pemilihan library ini menunjukkan pemahaman tentang ekosistem data science di Python, memilih alat yang tepat untuk tugas yang spesifik."
      ],
      "metadata": {
        "id": "pLeHYq4m5QrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 1: Menentukan ID Aplikasi ---\n",
        "app_id = 'com.duolingo' # ID aplikasi Duolingo di Google Play Store"
      ],
      "metadata": {
        "id": "FUsgIFoH5RBW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Metode yang digunakan:** Inisialisasi variabel.\n",
        "- **Alasan penggunaan:** Menyimpan ID aplikasi (`com.duolingo`) ke dalam sebuah variabel `app_id`. Ini adalah praktik *coding* yang baik karena:\n",
        "    1.  **Keterbacaan (Readability):** Kode menjadi lebih mudah dibaca.\n",
        "    2.  **Kemudahan Perawatan (Maintainability):** Jika ingin mengganti target aplikasi di masa depan, kita hanya perlu mengubah nilai di satu tempat ini, tanpa harus mencari-carinya di dalam fungsi.\n",
        "- **Insight dan Hasil yang didapat:** Secara eksplisit menetapkan **Duolingo** sebagai subjek analisis. Ini menjadi parameter kunci untuk langkah berikutnya."
      ],
      "metadata": {
        "id": "8sjH8lFB5XPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 2: Melakukan Scraping ---\n",
        "# proses ini mengambil semua ulasan yang tersedia.\n",
        "# menargetkan > 10.000 ulasan.\n",
        "print(f\"Memulai proses scraping untuk aplikasi: {app_id}...\")\n",
        "\n",
        "scrapreview = reviews_all(\n",
        "    app_id,\n",
        "    lang='id',           # Mengambil ulasan dalam Bahasa Indonesia\n",
        "    country='id',        # Menentukan negara sebagai Indonesia\n",
        "    sort=Sort.NEWEST,    # Mengurutkan dari yang terbaru untuk mendapatkan data terkini\n",
        "    count=15000          # Menargetkan 15.000 ulasan untuk memastikan mendapat >10.000 setelah dibersihkan\n",
        ")\n",
        "\n",
        "print(\"Proses scraping selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpj1H3-D5Xu3",
        "outputId": "dff36c59-5710-42f7-9421-4903ab62b08a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai proses scraping untuk aplikasi: com.duolingo...\n",
            "Proses scraping selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "`Memulai proses scraping untuk aplikasi: com.duolingo...`\n",
        "`Proses scraping selesai.`\n",
        "\n",
        "- **Metode yang digunakan:** Memanggil fungsi `reviews_all` dari library `google-play-scraper`.\n",
        "\n",
        "- **Alasan penggunaan:** Fungsi ini dipilih karena kemampuannya untuk mengambil ulasan dalam jumlah besar secara otomatis. Parameter yang digunakan sangat strategis dan relevan dengan tujuan proyek:\n",
        "    - `lang='id', country='id'`: Memastikan data yang diambil relevan dengan konteks bahasa Indonesia, yang akan menjadi fokus utama pada tahap NLP.\n",
        "    - `sort=Sort.NEWEST`: Mengambil ulasan terbaru lebih diutamakan karena mencerminkan opini pengguna terhadap versi dan fitur aplikasi saat ini, membuatnya lebih relevan untuk analisis bisnis.\n",
        "    - `count=15000`: Ini adalah **langkah krusial**. Dengan menargetkan jumlah yang lebih tinggi dari syarat minimal 10.000, kita mengantisipasi adanya data yang tidak relevan atau kotor yang mungkin akan dihapus pada tahap pembersihan. Ini menunjukkan perencanaan yang matang untuk memastikan dataset akhir tetap besar dan berkualitas.\n",
        "\n",
        "- **Insight dan Hasil yang didapat:**\n",
        "  - **Output teks** mengonfirmasi bahwa proses telah dimulai dan selesai tanpa error.\n",
        "  - Data mentah kini tersimpan dalam variabel `scrapreview`. Keberhasilan eksekusi pada hal ini adalah pencapaian utama."
      ],
      "metadata": {
        "id": "pzOReNmG5sT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 3: Konversi Hasil Scraping ke DataFrame ---\n",
        "# Membuat DataFrame dari hasil scraping untuk memudahkan manipulasi data\n",
        "df_reviews = pd.DataFrame(scrapreview)\n",
        "\n",
        "print(f\"Total ulasan yang berhasil di-scrape: {len(df_reviews)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_cb7YCA5sxp",
        "outputId": "c14316ff-33dd-4d74-a887-5d7f8de07f44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ulasan yang berhasil di-scrape: 192449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "`Total ulasan yang berhasil di-scrape: 192449`\n",
        "\n",
        "- **Metode yang digunakan:** Konversi struktur data menggunakan `pd.DataFrame()` dan pengecekan ukuran data dengan `len()`.\n",
        "- **Alasan penggunaan:** Mengubah data dari format list menjadi DataFrame adalah langkah standar untuk transisi dari pengumpulan data ke analisis data. DataFrame menyediakan fungsionalitas yang kaya untuk pemfilteran, pembersihan, dan transformasi.\n",
        "- **Insight dan Hasil yang didapat:**\n",
        "  - **Ini adalah insight paling signifikan dari notebook ini.** Output menunjukkan bahwa jumlah data yang berhasil diambil adalah **192.449 ulasan**, jauh melampaui target awal 15.000 dan syarat minimal 10.000.\n",
        "  - **Analisis Kritis:** Dataset sebesar ini memberikan fondasi yang sangat kuat untuk melatih model Deep Learning yang kompleks dan *data-hungry*. Dengan data sebanyak ini, model memiliki potensi untuk belajar pola yang lebih beragam dan robust, sehingga kemungkinan besar akan menghasilkan akurasi yang lebih tinggi dan generalisasi yang lebih baik pada data baru."
      ],
      "metadata": {
        "id": "XN-m2az25z18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 4: Memilih Kolom yang Relevan ---\n",
        "# Menggunakan kolom-kolom yang penting untuk analisis sentimen.\n",
        "relevant_columns = ['userName', 'content', 'score', 'at']\n",
        "df_relevant = df_reviews[relevant_columns]\n",
        "\n",
        "# 'userName': Nama pengguna\n",
        "# 'content': Isi ulasan/review\n",
        "# 'score': Rating yang diberikan (1-5)\n",
        "# 'at': Tanggal ulasan dibuat"
      ],
      "metadata": {
        "id": "94BMLYON50J2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Metode yang digunakan:** *Slicing* (pemotongan) DataFrame.\n",
        "- **Alasan penggunaan:** Efisiensi dan Relevansi. Tidak semua kolom yang disediakan oleh scraper (seperti `reviewId`, `userImage`, `replyContent`, dll.) diperlukan untuk analisis sentimen. Dengan memilih hanya kolom-kolom esensial (`userName`, `content`, `score`, `at`), kita:\n",
        "    1.  Mengurangi penggunaan memori.\n",
        "    2.  Menyederhanakan dataset, membuatnya lebih mudah untuk ditangani.\n",
        "    3.  Fokus pada fitur-fitur yang akan digunakan langsung: `content` untuk teks, `score` untuk pelabelan, `userName` dan `at` untuk analisis kontekstual jika diperlukan.\n",
        "- **Insight dan Hasil yang didapat:** Proses ini mengubah DataFrame mentah yang \"gemuk\" menjadi DataFrame yang \"ramping\" dan siap pakai, tanpa kehilangan informasi yang krusial untuk proyek."
      ],
      "metadata": {
        "id": "w6m0km7u6DxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Langkah 5: Simpan DataFrame ke File CSV ---\n",
        "# Menyimpan data bersih ke dalam file CSV.\n",
        "output_filename = 'dataset_duolingo.csv' # meberikan nama output file 'dataset_duolingo.csv'\n",
        "df_relevant.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Data ulasan telah berhasil disimpan ke dalam file: {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onkWGSWf6EDy",
        "outputId": "e7471f3a-9128-4a4f-82a6-5fbc0fab86cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ulasan telah berhasil disimpan ke dalam file: dataset_duolingo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "`Data ulasan telah berhasil disimpan ke dalam file: dataset_duolingo.csv`\n",
        "\n",
        "- **Metode yang digunakan:** Menyimpan DataFrame ke file menggunakan metode `.to_csv()`.\n",
        "- **Alasan penggunaan:** **Persistensi Data.** Tahap ini sangat penting untuk memisahkan alur kerja. Dengan menyimpan hasil scraping ke file `.csv`, kita tidak perlu menjalankan ulang proses scraping yang memakan waktu setiap kali kita ingin mengerjakan notebook pemodelan. Notebook pemodelan nantinya hanya perlu memuat file `dataset_duolingo.csv` ini, membuat proses pengembangan lebih cepat dan modular. Parameter `index=False` digunakan untuk mencegah Pandas menulis kolom indeks yang tidak perlu ke dalam file.\n",
        "- **Insight dan Hasil yang didapat:** Output teks mengonfirmasi bahwa aset data utama proyek telah berhasil dibuat dan disimpan. Proyek ini sekarang memiliki sumber data yang solid dan dapat digunakan kembali."
      ],
      "metadata": {
        "id": "tTcrjGT-6XD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan 5 baris pertama dari data yang disimpan\n",
        "print(\"\\nContoh 5 data pertama:\")\n",
        "print(df_relevant.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLPQNO1k6Xq8",
        "outputId": "c6150775-dd53-4f7f-f8ef-3947dffcce94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh 5 data pertama:\n",
            "          userName                                            content  score  \\\n",
            "0  Pengguna Google                              karena pendukung lgbt      1   \n",
            "1  Pengguna Google  dari pelajaran, style gambar, fungsi, dan lain...      3   \n",
            "2  Pengguna Google                                               nice      5   \n",
            "3  Pengguna Google                                        sangat seru      5   \n",
            "4  Pengguna Google  aplikasi ini bagus, mengajarkan beberapa bahas...      5   \n",
            "\n",
            "                   at  \n",
            "0 2025-06-04 06:10:55  \n",
            "1 2025-05-10 08:39:03  \n",
            "2 2025-05-10 08:22:54  \n",
            "3 2025-05-10 08:20:12  \n",
            "4 2025-05-10 08:19:15  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "Tabel yang menampilkan 5 baris pertama dari data.\n",
        "\n",
        "- **Metode yang digunakan:** Inspeksi data menggunakan metode `.head()`.\n",
        "- **Alasan penggunaan:** Sebagai langkah verifikasi akhir atau *sanity check*. Ini memungkinkan kita untuk melihat secara langsung sampel dari data yang telah disimpan, memastikan bahwa struktur kolom, tipe data, dan isinya sesuai dengan yang diharapkan.\n",
        "- **Insight dan Hasil yang didapat:**\n",
        "  - **Output tabel** menunjukkan data yang bersih dan terstruktur dengan benar dalam 4 kolom yang telah dipilih.\n",
        "  - Kita bisa melihat contoh nyata dari konten ulasan: ada yang singkat (`nice`), ada yang lebih panjang dan deskriptif (`aplikasi ini bagus...`).\n",
        "  - Terlihat juga variasi skor (1, 3, 5), yang akan menjadi dasar pelabelan sentimen (Negatif, Netral, Positif).\n",
        "  - Format tanggal pada kolom `at` terlihat konsisten. Secara keseluruhan, data ini siap untuk tahap selanjutnya yaitu preprocessing dan pemodelan."
      ],
      "metadata": {
        "id": "cpt5p9XT6Y5_"
      }
    }
  ]
}